{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run 2020-07-02 07:31\n"
     ]
    }
   ],
   "source": [
    "# This notebook imports COVID-19 data from usfacts.org and merges it with John Hopkins data\n",
    "# to create a dataset with following columns.\n",
    "\n",
    "# Target Features:\n",
    "# Date\n",
    "# Region (County or AFB): This is the county, burough, parish, etc.\n",
    "# Province (State): This is the state but we use province in case we add in international data\n",
    "# Country\n",
    "# Total_Confirmed :Total cumulative confirmed cases\n",
    "# Total_Deaths: The total cumulative reported deaths\n",
    "# Total_Recoveries: The total cumulative people who were confirmed and later recovered\n",
    "# Longitude (TODO: Need to add back in)\n",
    "# Latitude (TODO: Need to add back in)\n",
    "# Population\n",
    "# Daily_Confirmed: Total confirmed cases by day\n",
    "# Daily_Deaths: Total reported deaths by day\n",
    "# Proportional_Confirmed: Total confirmed bases by day per 100,000 people\n",
    "# Proportional_Deaths: Total reported deaths by day per 100,000 people\n",
    "# Days_Confirmed: Number of days since the daily confirmed cases exceeded a given limit (limits defined below)\n",
    "# Days_Deaths: Number of days since the daily reported deaths exceeded a given limit (limits defined below)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime, date, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os.path\n",
    "print ('Last run ' + datetime.now().strftime('%Y-%m-%d %H:%M'))\n",
    "\n",
    "# TODO: Add FIPS, Latitude and Longitude columns back into the final dataset\n",
    "\n",
    "# REVISION HISTORY\n",
    "revision = 'R05C03'\n",
    "# R02C00: Added the corrected population data by county (data source was US Census website)\n",
    "\n",
    "# R03C00: Added revision number to the export filenames so we can track the source notebook\n",
    "#        Added proportional metrics (e.g. confirmed cases per 100,000 people)\n",
    "# R04C00: Added data for 01/21 through 03/21 from usafacts.org\n",
    "#        Removed the Total_Recovered, Total_Active Columns, Longitude and Latitude columns because they are not in the usafacts.org dataset. Will add back in later.\n",
    "# R04C01: Added Total_Recoveries data from Wikipedia for counties in New York. Data for other states to be added later.\n",
    "# R05C00: Added Total_Recoveries data from Wikipedia for counties in New Jersey, Illinois, Maine, Maryland and South Dakota. No other states on Wikipedia have recoveries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1A: Download the data from https://usafacts.org/visualizations/coronavirus-covid-19-spread-map/\n",
    "# Download the latest file containing the confirmed cases \n",
    "url = 'https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_confirmed_usafacts.csv'\n",
    "fn = 'DATA/USAFACTS/CONFIRMED/' + date.today().strftime('%Y-%m-%d') + '-USAFACTS-CONFIRMED.csv'\n",
    "urllib.request.urlretrieve(url, fn)\n",
    "\n",
    "# Read the data from the file we downloaded\n",
    "cdf = pd.read_csv(fn)\n",
    "\n",
    "# Remove spurious data records from the list\n",
    "cdf = cdf.drop(cdf.columns[len(cdf.columns)-1], axis=1)\n",
    "remove_list = ['Statewide Unallocated','Grand Princess Cruise Ship']\n",
    "cdf = cdf[~cdf['County Name'].isin(remove_list)]\n",
    "\n",
    "# Merge the population data into the dataframe\n",
    "states = pd.read_csv('DATA/REFERENCE/us-state-abbreviations.csv')\n",
    "states = states.rename(columns={'Abbreviation':'State','Name':'Province'})\n",
    "cdf = pd.merge(cdf, states[['State', 'Province']], on='State')\n",
    "\n",
    "# Clean up the county names\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' County and City'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' County'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' City and Borough'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' Borough'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' Census Area'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' Parish'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' city'], value='')\n",
    "cdf['County Name'] = cdf['County Name'].replace(regex=[' City'], value='')\n",
    "\n",
    "# Reset the index\n",
    "cdf = cdf.reset_index(drop=True)\n",
    "\n",
    "# Write the data to file just for testing\n",
    "cdf.to_csv('DATA/TEST/' + date.today().strftime('%Y-%m-%d') + '-USAFACTS-CONFIRMED-MUNGED-v3.csv',index=False)\n",
    "\n",
    "# Download the latest file containing the number of deaths\n",
    "url = 'https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_deaths_usafacts.csv'\n",
    "fn = 'DATA/USAFACTS/DEATHS/' + date.today().strftime('%Y-%m-%d') + '-USAFACTS-DEATHS.csv'\n",
    "urllib.request.urlretrieve(url, fn)\n",
    "\n",
    "# Read the data from the file we downloaded\n",
    "ddf = pd.read_csv(fn)\n",
    "\n",
    "# Remove \n",
    "ddf = ddf[~ddf['County Name'].isin(remove_list)]\n",
    "\n",
    "# Merge the population data into the dataframe\n",
    "ddf = pd.merge(ddf, states[['State', 'Province']], on='State')\n",
    "\n",
    "# Clean up the county names\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' County and City'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' County'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' City and Borough'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' Borough'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' Census Area'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' Parish'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' city'], value='')\n",
    "ddf['County Name'] = ddf['County Name'].replace(regex=[' City'], value='')\n",
    "\n",
    "# Reset the index\n",
    "ddf = ddf.reset_index(drop=True)\n",
    "\n",
    "# Create an empty dictionary with the standard columns\n",
    "df1 = pd.DataFrame(columns=['Date','Region','Province','Country','Total_Confirmed','Total_Deaths'])\n",
    "dict1 = {}\n",
    "\n",
    "# Set the default country since all the data is for the US\n",
    "Country = 'US'\n",
    "\n",
    "i = 0\n",
    "# loop through the rows in the columnar dataset\n",
    "for row in range(0,len(cdf)):\n",
    "    #Loop through the columns and add the confirmed cases to the new file\n",
    "    for col in range(4,len(cdf.columns)-1):\n",
    "        dict1[i] = {'Date':cdf.columns[col],'Region':cdf.iat[row,1],'Province':cdf.iat[row,cdf.columns.get_loc(\"Province\")],'Country':Country,'Total_Confirmed':cdf.iat[row,col],'Total_Deaths':ddf.iat[row,col]}\n",
    "        i += 1\n",
    "\n",
    "# Read the data into the datafram\n",
    "df1 = pd.DataFrame.from_dict(dict1,orient='index')\n",
    "\n",
    "# Convert Total_Deaths column to int because for some reason it comes in as float\n",
    "df1.Total_Deaths = df1.Total_Deaths.astype(int)\n",
    "\n",
    "# Write the data to file for easy use elsewhere\n",
    "df1.to_csv('DATA/USAFACTS/' + date.today().strftime('%Y-%m-%d') + '-USAFACTS-MERGED-' + revision + '.csv',index=False)\n",
    "\n",
    "# STEP 1B: Download all the JHU daily files to local disk just so we have the records\n",
    "\n",
    "# We need create a list of string dates from 01/22 through today\n",
    "start_date = date(2020,1,22)\n",
    "end_date = date.today()\n",
    "period = end_date - start_date\n",
    "datelist = []\n",
    "\n",
    "for dindex in range(period.days):\n",
    "    currentdate = start_date + timedelta(days=dindex)\n",
    "    datelist.append(currentdate.strftime('%m-%d-%Y'))\n",
    "\n",
    "# Note: This code will overwrite any files that were downloaded earlier\n",
    "for d in datelist:\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/' + d + '.csv'\n",
    "    fn = 'DATA/JHU/DAILY/' + d + '-JHU-DAILY.csv'\n",
    "    if not (os.path.exists(fn)):\n",
    "        urllib.request.urlretrieve(url, fn)\n",
    "        \n",
    "# STEP 2: Load data from 03/22 through today\n",
    "#Note: The JHU data before 03/22 does not have information by county\n",
    "\n",
    "# Setup the date range for the first period\n",
    "start_date = date(2020,3,22)\n",
    "end_date = date.today()\n",
    "period = end_date - start_date\n",
    "\n",
    "# Create an empty list to hold the range of date strings\n",
    "drng2 = []\n",
    "\n",
    "# Loop through the dae range and populate the list of date strings\n",
    "for dindex in range(period.days):\n",
    "    currentdate = start_date + timedelta(days=dindex)\n",
    "    drng2.append(currentdate.strftime('%m-%d-%Y'))\n",
    "\n",
    "# Create an empty dataframe with the standard columns\n",
    "df2 = pd.DataFrame(columns=['Date','Region','Province','Country','Total_Confirmed','Total_Deaths'])\n",
    "    \n",
    "# Loop through the date range and put all the date into one file\n",
    "for d in drng2:\n",
    "    # Load the data file for this particular date in the loop\n",
    "    fn = 'DATA/JHU/DAILY/' + d + '-JHU-DAILY.csv'\n",
    "    rdf = pd.read_csv(fn)\n",
    "    \n",
    "    # Rename and remove columns\n",
    "    rdf = rdf.rename(columns={\"Admin2\":\"Region\", \"Province_State\":\"Province\", \"Country_Region\":\"Country\", \"Confirmed\":\"Total_Confirmed\", \"Deaths\":\"Total_Deaths\"})\n",
    "    # We are dropping Latitude and Longitude because it isn't in the USAFACTS data files, but we might want to merge back in later\n",
    "    rdf = rdf.drop(['FIPS','Last_Update','Combined_Key','Recovered','Active','Lat','Long_'],axis=1)\n",
    "    \n",
    "    \n",
    "    # Set the date corresponding to the date of the file (i.e. filename)\n",
    "    rdf[\"Date\"] = d\n",
    "    \n",
    "    # Append the data to the master data file for this range\n",
    "    df2 = df2.append(rdf)\n",
    "    \n",
    "# Convert to datetime so it sorts by date, not by string\n",
    "df2.Date = pd.to_datetime(df2.Date)\n",
    "\n",
    "# Filter out all countries except the US\n",
    "df2 = df2.sort_values(by=['Date'])\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df2 = df2[df2.Country == 'US']\n",
    "\n",
    "# Drop all records that contain data for individual cities in the Province field (they contain a comma)\n",
    "df2 = df2.sort_values(by=['Date'])\n",
    "df2 = df2.reset_index(drop=True)\n",
    "df2 = df2.drop(df2[df2.Province.str.contains(',')].index)\n",
    "\n",
    "# Remove the extraneous province data\n",
    "df2 = df2.sort_values(by=['Date'])\n",
    "df2 = df2.reset_index(drop=True)\n",
    "remove_list = ['Chicago','Unassigned Location (From Diamond Princess)','Grand Princess Cruise Ship','Grand Princess','Diamond Princess','Wuhan Evacuee','Recovered','Northern Mariana Islands','American Samoa','Guam','United States Virgin Islands','Virgin Islands','US','Puerto Rico']\n",
    "df2 = df2[~df2.Province.isin(remove_list)]\n",
    "df2 = df2.sort_values(by=['Date'])    \n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "# Merge in the USAFACTS dataset for data prior to 03/22\n",
    "df1.Date = pd.to_datetime(df1.Date)\n",
    "df1b = df1[df1.Date < pd.to_datetime(start_date)]\n",
    "df2 = df2.append(df1b)\n",
    "df2 = df2.sort_values(by=['Date'])    \n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "# Write the data to disk\n",
    "dstr = date.today().strftime('%Y-%m-%d')\n",
    "df2.to_csv('DATA/' + dstr + '-COVID-19-ALL-' + revision + '.csv',index=False)\n",
    "\n",
    "# STEP 3: Load the US Census data by population and clean it up so we can merge the population data into data set\n",
    "popdf = pd.read_csv('DATA/REFERENCE/us-population-by-county.csv')\n",
    "popdf = popdf.drop(popdf[popdf.Province == popdf.Region].index)\n",
    "\n",
    "#Remove the word \"county\" from the end of the Regions\n",
    "popdf['Region'] = popdf['Region'].replace(regex=[' County'], value='')\n",
    "popdf['Region'] = popdf['Region'].replace(regex=[' City and Borough'], value='')\n",
    "popdf['Region'] = popdf['Region'].replace(regex=[' Borough'], value='')\n",
    "popdf['Region'] = popdf['Region'].replace(regex=[' Census Area'], value='')\n",
    "popdf['Region'] = popdf['Region'].replace(regex=[' Parish'], value='')\n",
    "popdf['Region'] = popdf['Region'].replace(regex=[' city'], value='')\n",
    "popdf.to_csv('DATA/REFERENCE/us-population-by-county-' + revision + '.csv',index=False)\n",
    "\n",
    "# STEP 4: Filter the data to only the US and calculate the daily metrics\n",
    "\n",
    "# Create a dataframe to hold the data with the daily metrics\n",
    "df3 = pd.DataFrame(columns=['Date','Region', 'Province', 'Country', 'Total_Confirmed', 'Total_Deaths'])\n",
    "\n",
    "# Get a list of unique states (provinces)\n",
    "provinces = pd.DataFrame(df2.Province.unique())\n",
    "provinces = provinces.rename(columns={0:\"Province\"})\n",
    "\n",
    "# Set the outbreak limits\n",
    "death_limit = 0 #Assumption: Once the daily deaths exceeds this limit for the first time, community spread has started\n",
    "confirmed_limit = 30 #Assumption: Once the confirmed cases exceeds this limit for the first time, community spread has started\n",
    "\n",
    "# Loop through the states and calculate the daily metrics for each county\n",
    "for pindex, p in provinces.iterrows():\n",
    "    # Get a temporary data frame for this province\n",
    "    sdf = df2[df2.Province == p.Province]\n",
    "    sdf = sdf.sort_values(by=['Date'])\n",
    "    sdf = sdf.reset_index(drop=True)\n",
    "    \n",
    "    # Filter the population down to this province\n",
    "    rpopdf = popdf[popdf.Province == p.Province]\n",
    "    rpopdf = rpopdf.reset_index(drop=True)\n",
    "    \n",
    "    # Merge the population data into the dataframe\n",
    "    sdf = pd.merge(sdf, rpopdf[['Region', 'Population']], on='Region')\n",
    "\n",
    "    # Get a list of unique regions (counties) for this province\n",
    "    regions = pd.DataFrame(sdf.Region.unique())\n",
    "    regions = regions.rename(columns={0:\"Region\"})\n",
    "\n",
    "    for rindex, r in regions.iterrows():\n",
    "        # Get a temporary dataframe for this region\n",
    "        rdf = sdf[sdf.Region == r.Region]\n",
    "        rdf = rdf.sort_values(by=['Date'])\n",
    "        rdf = rdf.reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the Daily Confirmed Cases\n",
    "        rdf['Daily_Confirmed'] = rdf.Total_Confirmed - rdf.Total_Confirmed.shift(1)\n",
    "        rdf['Daily_Deaths'] = rdf.Total_Deaths - rdf.Total_Deaths.shift(1)\n",
    "        \n",
    "        # Calculation the daily metrics per 100,000 people\n",
    "        rdf['Proportional_Confirmed'] = rdf.Daily_Confirmed / rdf.Population * 100000\n",
    "        rdf['Proportional_Deaths'] = rdf.Daily_Deaths / rdf.Population * 100000\n",
    "\n",
    "        # Calculate the number of day from when the daily confirmed cases exceeds the outbreak limit\n",
    "        if rdf.Daily_Confirmed.max() > confirmed_limit:\n",
    "            # Get the first date when the number of confirmed cases exceeded 30\n",
    "            cstart = rdf[rdf.Daily_Confirmed > confirmed_limit].iat[0,0]\n",
    "\n",
    "            # Calculate the number of days from the start date (i.e. set the Days column values)\n",
    "            rdf['Days_Confirmed'] = (rdf.Date - cstart).dt.days\n",
    "        else: # If the maximum confirmed cases has not yet exceeded the limit then set the days to 0\n",
    "            rdf['Days_Confirmed'] = 0\n",
    "\n",
    "        # Calculate the number of day from when the daily confirmed cases exceeds the outbreak limit\n",
    "        if rdf.Daily_Deaths.max() > death_limit:\n",
    "            # Get the first date when the number of confirmed cases exceeded 30\n",
    "            dstart = rdf[rdf.Daily_Deaths > death_limit].iat[0,0]\n",
    "\n",
    "            # Calculate the number of days from the start date (i.e. set the Days column values)\n",
    "            rdf['Days_Deaths'] = (rdf.Date - dstart).dt.days\n",
    "        else: # If the maximum confirmed cases has not yet exceeded the limit then set the days to 0\n",
    "            rdf['Days_Deaths'] = 0\n",
    "            \n",
    "        # Drop the first day of data because the calculations will be off\n",
    "        rdf = rdf.sort_values(by=['Date'])\n",
    "        rdf = rdf.reset_index(drop=True)\n",
    "        rdf = rdf.drop(index=0)\n",
    "\n",
    "        # Append to new data to the dataframe\n",
    "        df3 = df3.append(rdf)\n",
    "\n",
    "# Sort and reset the new dataframe\n",
    "df3 = df3.sort_values(by=['Date'])\n",
    "df3 = df3.reset_index(drop=True)\n",
    "        \n",
    "# Clean up the datatypes and precision for the new columns\n",
    "df3.Daily_Confirmed = df3.Daily_Confirmed.fillna(0)\n",
    "df3.Daily_Confirmed = df3.Daily_Confirmed.astype(int)\n",
    "df3.Daily_Deaths = df3.Daily_Deaths.fillna(0)\n",
    "df3.Daily_Deaths = df3.Daily_Deaths.astype(int)\n",
    "df3.Days_Confirmed = df3.Days_Confirmed.astype(int)\n",
    "df3.Days_Deaths = df3.Days_Deaths.astype(int)\n",
    "df3.Population = df3.Population.astype(int)\n",
    "df3 = df3.round({'Proportional_Confirmed':4,'Proportional_Deaths':4})\n",
    "df3.to_csv('DATA/' + date.today().strftime('%Y-%m-%d') + '-COVID-19-US-' + revision + '.csv',index=False)\n",
    "df3.to_csv('DATA/covid-19-data-usafacts+jhu-us-bycounty.csv',index=False)\n",
    "df3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
